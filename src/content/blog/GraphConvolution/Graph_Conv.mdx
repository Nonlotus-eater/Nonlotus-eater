---
title: "GConv-S 图卷积原理及模型应用"
description: "Graph Convolution & its application"
publishDate: 2025-11-17
tags: ["machine-learning", "deep-learning", "fine-tuning", "lora", "large-language-models", "parameter-efficient"]
comment: true
---

import { Aside } from 'astro-pure/user'
import remarkMath from 'remark-math'
import rehypeKatex from 'rehype-katex'

{/* console.log(
  String(
    await compile(await fs.readFile('Graph_Conv.mdx'), {
      rehypePlugins: [rehypeKatex],
      remarkPlugins: [remarkMath]
    })
  )
) */}

## 图卷积发展简介

图卷积网络（Graph Convolutional Networks, GCNs）的起源源于深度学习在规则欧氏数据（如图像）上取得的巨大成功与图数据这一非欧氏结构处理需求之间的矛盾。传统卷积神经网络（CNN）在图像等具有规则网格结构的数据上表现出色，其核心在于利用局部连接和权值共享机制有效提取空间特征。然而，当面对社交网络、分子结构、知识图谱等不规则图数据时，节点连接关系的任意性和拓扑结构的不规则性使得直接应用CNN的卷积操作变得困难。

在图卷积的早期探索中，研究者提出了两种主要的技术路线：

### 空间方法

试图将图数据映射到规则的空间坐标系中，然后借鉴图像处理的方式进行分块卷积操作。

具体而言，通过为图节点赋予空间坐标，将图结构转换为类似于图像的规则网格表示，再应用传统的CNN架构。

缺点：
   
   1. 无法有效处理边权
   2. 导致图拓扑结构失真（强制将非欧氏空间的图数据嵌入到欧氏空间） 
   3. 无法保证卷积操作的平移不变性

### 频域方法

在频域中定义图卷积操作。

核心思想：利用图拉普拉斯矩阵的特征分解进行图傅里叶变换，将图信号从空间域转换到频域，然后在频域中应用可学习的滤波器进行卷积，最后通过逆变换回到空间域。

## 数学原理

### 模型定义

图卷积定义的函数如下：

$$
H^{(l+1)} = \sigma(\hat{D}^{-\frac{1}{2}} \hat{A} \hat{D}^{-\frac{1}{2}} H^{(l)} W^{(l)})
$$

它的物理含义是：将融入前一层自身信息和邻居节点信息的图加权求和后进行线性变换$W$和非线性变换$\sigma$。
其中$\hat{A} = I + A$, $\hat{D}^{-\frac{1}{2}} \hat{A} \hat{D}^{-\frac{1}{2}}$是对$\hat{A}$的对称归一化。

<Aside type="tip" title="为什么给邻接矩阵添加自环">
在最简单的情况下，我们将邻接矩阵和每层的嵌入直接相乘，即$AH$，根据矩阵的运算公式，我们可以发现它的意义就是快速将相邻的节点的信息相加得到自己下一层的输入。

然而，这样的操作虽然获得了周围节点的信息，但是节点本身的信息却没了，因此需要添加自环：
$$
\hat{A} = I + A
$$
</Aside>

<Aside type="tip" title="为什么对邻接矩阵进行对称归一化">
如果每层都将采用邻接矩阵和每层的嵌入直接相乘的方法，很明显每层的输出会不断增大，即特征向量$X$的scale与输入的差别会越来越大。
所以我们可以将$A$的每一行除以每一行的和，这里每一行的和就是每个节点的度，最终我们可以得到归一化的$A$:
$$
A = D^{-1} A , A_{ij} = \frac{A_{ij}}{d_i}
$$
在实际运用中，图卷积采用了对称归一化：
$$
A = D^{-\frac{1}{2}} A D^{-\frac{1}{2}} , A_{ij} = \frac{A_{ij}}{\sqrt{d_i} \sqrt{d_j}}
$$
</Aside>

将这两个技巧结合起来，可以得到：
$$
H^{(l+1)} = f(H^{(l)}, A) = \sigma(\hat{D}^{-\frac{1}{2}} \hat{A} \hat{D}^{-\frac{1}{2}} H^{(l)} W^{(l)})
$$

其中$\hat{A} = I + A$, $\hat{D}$是$\hat{A}$的度矩阵。

### 数学推导

如果需要理解图卷积，我们需要先理解图拉普拉斯矩阵和图傅里叶变换：

1. 图拉普拉斯矩阵 $Graph Laplacian Matrix L(G)$

我们可以用多种方式表示一个图，可以用邻接矩阵，也可以用关联矩阵$(Incidence Matrix)$：

<Aside type="note" title="关联矩阵">
在关联矩阵中，每一行表示一个边，每一列表示一个节点。每一行中，边的节点的起点用记为1，边的终点记为-1，我们将这个matrix记为$C$

![关联矩阵](/images/GraphConvolution/Incidence.png)
</Aside>

图拉普拉斯矩阵具有以下性质：
<Aside type="note" title="图拉普拉斯矩阵">
1. L(G)具有以下性质：

(1)L(G)是对称矩阵

(2)L(G)的特征值 $\Lambda$ 非负

(3)L(G)的特征矩阵 $U$ 正交: $U^T U = I$

2. 图拉普拉斯阵与关联矩阵、度矩阵、邻接矩阵的关系：
$$
L(G) = C^T C = D - A
$$

![L=D-A的计算方式](/images/GraphConvolution/Laplacian.png)

图拉普拉斯矩阵可被其特征值$\Lambda$和特征向量$U$表示如下：
$$
L = U \Lambda U^T
$$

3. 对称化的拉普拉斯矩阵：
$$
L_{sym} = D^{-\frac{1}{2}} L D^{-\frac{1}{2}} = I - D^{-\frac{1}{2}} A D^{-\frac{1}{2}}
$$

</Aside>

2. 图傅里叶变换

在傅里叶变换中，卷积定理定义了两个函数的卷积方式：

$$
\mathscr{f} * g = \mathcal{F}^{-1}\{\mathcal{F}\{\mathscr{f}\} \cdot \mathcal{F}\{g\}\}
$$

在图傅里叶变换中，图傅里叶变换的变换基被定义为：
$$
\mathcal{F}(x) = U^T x
$$
逆变换基被定义为：
$$
\mathcal{F}^{-1} (x) = U x
$$

那么按照卷积定理，图卷积的公式为：

$$
\begin{aligned}
g * \alpha &= \mathcal{F}^{-1}\{\mathcal{F}\{g\} \cdot \mathcal{F}\{\alpha\}\} \\
&= U(U^T g \cdot U^T \alpha)
\end{aligned}
$$
这里的$g$可以被看做是一个laplacian的函数$g(L)$，是图的特征滤波器，作用一次相当于传播一次周围邻居节点的信息。

又因为$U^T L = U^T U \Lambda U^T = \Lambda U^T$，以我们可以把 $U^T g$ 看成是图拉普拉斯矩阵的特征值的函数 $g_{\theta} (\Lambda) = diag(\theta)$  , 参数为 $\theta$ 。

所以图卷积可以被定义成如下公式：

$$
g_{\theta} * x = U g_{\theta}(\Lambda)U^T x
$$

3. 图卷积

$$
g_{\theta} * x = U g_{\theta}(\Lambda)U^T x
$$

其中$g_{\theta}$表示图卷积算子，$U$是输入数据的Laplace矩阵的特征向量矩阵，$\Lambda$是特征值对角阵。

故$\theta(\Lambda)$可以理解为图数据的频域滤波操作，整个公式可以理解为对图数据进行频域滤波后再进行图傅里叶变换。

其中对称归一化的Laplace矩阵满足以下性质：

$$ 
L = I_n - D^{-\frac{1}{2}} W D^{-\frac{1}{2}}
$$

其中W为输入图的邻接矩阵，D为输入图的度矩阵。

然而，$U \theta(\Lambda)U^T$的时间复杂度达到了$O(n^3)$，运算代价太大，最好的简化方法是将$\theta(\Lambda)$当成多项式进行计算，此时需要有一种能够近似频域滤波图的多项式，这里图卷积采用了Chebyshev多项式进行近似。

<Aside type="note" title="Chebyshev多项式">
图卷积使用到多项式的性质如下：

$$
对\forall x \in [-1,1], \exists f(x), 满足 f(x) \approx \Sigma_{k=0}^\infty \theta_k T_k(x)
$$
其中
$$
T_0(x)=1, T_1(x)=x, T_2(x)=2xT_1(x)-T_0(x),...,T_{n+1}(x) = 2xT_n(x)-T_{n-1}(x)
$$
</Aside>

由Chebyshev多项式的性质，若要将$\theta(\Lambda)$当成多项式进行计算，需将$\Lambda$映射到$[-1,1]$，故对$\Lambda$进行如下操作：

$$
式一：\tilde{\Lambda} = 2 \cdot \frac{\Lambda}{\lambda_{max}} - I_n
$$

故而$\theta_{*g}(x)$可以进行如下演算：

$$
\begin{aligned}
\theta_{*g}(x) &= U \theta(\Lambda) U^T x \\
&= U \left[\sum_{k=0}^\infty \theta_k T_k(\tilde{\Lambda})\right] U^T x \\
&= \sum_{k=0}^\infty \theta_k \left[U T_k(\tilde{\Lambda}) U^T\right] x \\
&= \sum_{k=0}^\infty \theta_k T_k(U \tilde{\Lambda} U^T) x
\end{aligned}
$$

带入式一，可得：

$$
\begin{aligned}
\text{原式} &= \sum_{k=0}^\infty \theta_k \left[T_k\left(2 \cdot \left(\frac{L}{\lambda_{\max}} - I_n\right)\right)\right] x \\
    &= \sum_{k=0}^\infty \theta_k T_k(\tilde{L}) x
\end{aligned}
$$

此时$T_k(\tilde{L})$是定值，可以直接计算。其中为什么$U T_k(\tilde{\Lambda}) U^T = T_k(\tilde{L})$, 可通过数学归纳法进行证明。
（要打的太多了，写不动了，先用纸质的证明看看叭）

![证明](/images/GraphConvolution/Prove1.jpg)